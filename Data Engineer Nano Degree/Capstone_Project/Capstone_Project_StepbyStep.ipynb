{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19 Data Collection\n",
    "\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "\n",
    "### Project Summary\n",
    "During the pandemic of 2020, the ability to track and use data to ensure the safety of others is/was crucial. This project takes some of that data collected by others, and houses it in a cluster on AWS. The benefit of this collection would allow others to use this data in its normalized format to perform analytical queries. For ease of use during this ETL process the data was downloaded from the corresponding sites and loaded directly onto my AWS S3 bucket. The data itself was pulled from the two source listed below. Additionally, when combined in tabular form this data was over a million rows meeting the requirement of the project. I will be limiting my scope to that of the United States, which will be filtered out later during the ETL process.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Using Version Python 3.7.6\n",
    "# This imports all the necessary packages that will be used during this main ETL script\n",
    "import pandas as pd\n",
    "import json  \n",
    "import os\n",
    "import urllib.request\n",
    "import boto \n",
    "import sys\n",
    "import boto3\n",
    "import configparser\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sql_queries import counting_audit_queries\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "\n",
    "# Gets configuration file used here for S3 Bucket specifically\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "# Gets current directory location used several time through this project\n",
    "currentDir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope and Description\n",
    "##### Google Community Mobility Reports\n",
    "This data aims to provide insights into how movement has shifted and changed throughout the pandemic. The data was collected through Google's store of location data throughout the world and was downloaded as a CSV file. In tabular form this data is approximately 470,000 rows of data (at the time of ingestion.) If you would like to know more about this data set, please visit Google's page [here.](https://www.google.com/covid19/mobility/)\n",
    "\n",
    "\n",
    "##### AWS Covid 19 Data Lake\n",
    "This data is an open source data lake collected by various parties and loaded onto AWS. The data itself contains population, confirmed and death cases data. I used tableau's collection of three Json files. In tabular form this data would be approximately 940,000 rows of data (at the time of ingestion.) It appears to have been gathered by John Hopkins University using a robotic collect and store methodology. You can find out more about the collection means by John Hopkins University [here.](https://github.com/CSSEGISandData/COVID-19) Additionally, you can find the actual data lake where the Json files were downloaded at [this AWS location.](https://dj2taa9i652rf.cloudfront.net/)\n",
    "\n",
    "\n",
    "##### Tools and plan\n",
    "I plan to take the data from my local computer and load the files into S3. From there I will be doing some light transformation on my local machine just for formatting purposes. The rest of the transformation process will take place in AWS Redshift (cloud data warehouse solution.) The data will be in a normalized for hypothetical data analysis purposes. \n",
    "\n",
    "##### Getting Started\n",
    "To get started I downloaded the files above and placed them in my Jupyter Notebook directory. I have them placed in this folder for convenience. \n",
    "\n",
    "* The steps to find the John Hopkins data once clicking the link is to navigate to the following directory: covid19-lake/ tableau-jhu / json  -- Note: I would discourage downloading this data as it is subject to column change often and should use what is already downloaded. You will see later where column adjustment had to be made\n",
    "* To download the Google data from the link mentioned above, simply click the \"Download global CSV\" button\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identifying data quality issues, like missing values, duplicate data, etc....\n",
    "\n",
    "#### Google Mobility Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_region_code</th>\n",
       "      <th>country_region</th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>sub_region_2</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_region_code        country_region sub_region_1 sub_region_2  \\\n",
       "0                  AE  United Arab Emirates          NaN          NaN   \n",
       "1                  AE  United Arab Emirates          NaN          NaN   \n",
       "2                  AE  United Arab Emirates          NaN          NaN   \n",
       "3                  AE  United Arab Emirates          NaN          NaN   \n",
       "4                  AE  United Arab Emirates          NaN          NaN   \n",
       "\n",
       "         date  retail_and_recreation_percent_change_from_baseline  \\\n",
       "0  2020-02-15                                                0.0    \n",
       "1  2020-02-16                                                1.0    \n",
       "2  2020-02-17                                               -1.0    \n",
       "3  2020-02-18                                               -2.0    \n",
       "4  2020-02-19                                               -2.0    \n",
       "\n",
       "   grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "0                                                4.0   \n",
       "1                                                4.0   \n",
       "2                                                1.0   \n",
       "3                                                1.0   \n",
       "4                                                0.0   \n",
       "\n",
       "   parks_percent_change_from_baseline  \\\n",
       "0                                 5.0   \n",
       "1                                 4.0   \n",
       "2                                 5.0   \n",
       "3                                 5.0   \n",
       "4                                 4.0   \n",
       "\n",
       "   transit_stations_percent_change_from_baseline  \\\n",
       "0                                            0.0   \n",
       "1                                            1.0   \n",
       "2                                            1.0   \n",
       "3                                            0.0   \n",
       "4                                           -1.0   \n",
       "\n",
       "   workplaces_percent_change_from_baseline  \\\n",
       "0                                      2.0   \n",
       "1                                      2.0   \n",
       "2                                      2.0   \n",
       "3                                      2.0   \n",
       "4                                      2.0   \n",
       "\n",
       "   residential_percent_change_from_baseline  \n",
       "0                                       1.0  \n",
       "1                                       1.0  \n",
       "2                                       1.0  \n",
       "3                                       1.0  \n",
       "4                                       1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the Google Global Mobility Report\n",
    "# The CSV is huge, in order to view the data complete I chunked the data into a dataframe\n",
    "fullGoogleData = pd.DataFrame([])\n",
    "\n",
    "for df in pd.read_csv('Global_Mobility_Report.csv', iterator=True, chunksize=1000):\n",
    "    fullGoogleData = fullGoogleData.append(df)\n",
    "fullGoogleData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows present in Google Global Mobility Report: 477322\n",
      "Total number of rows deduplicated: 0\n"
     ]
    }
   ],
   "source": [
    "# First checking row count and then de duplicating to see if this will be a necessary part of clean up later\n",
    "rowNum = len(fullGoogleData.index)\n",
    "print('Number of rows present in Google Global Mobility Report:', rowNum)\n",
    "fullGoogleData.drop_duplicates() \n",
    "rowNum2 = len(fullGoogleData.index)\n",
    "print('Total number of rows deduplicated:', rowNum-rowNum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google Mobility Data Assessment\n",
    "\n",
    "Generally speaking, the Google data largely doesn't have any data quality issues. The primary concern is that there are many null values which can produce problem in data analysis. I will be removing a few rows with a null value upon insert into RedShift. Additionally I will handle other nulls to ensure that a good composite key will be enforced, which also helps analytics queries later. The \"County\" string at the end of the county column will also need handled to ensure a good composite key later and a match between the two tables. This table is a pretty straight forward load. I will be filtering by the United State as part of my scope which will be part the transformation process of the ETL SQL logic.\n",
    "\n",
    "#### John Hopkins Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_Type</th>\n",
       "      <th>Cases</th>\n",
       "      <th>Difference</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>Combined_Key</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Prep_Flow_Runtime</th>\n",
       "      <th>FIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confirmed</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5/22/2020</td>\n",
       "      <td>Western Sahara</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td>Western Sahara</td>\n",
       "      <td>24.215500</td>\n",
       "      <td>-12.885800</td>\n",
       "      <td>6/4/2020 12:18:58 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confirmed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2/3/2020</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>46.818200</td>\n",
       "      <td>8.227500</td>\n",
       "      <td>6/4/2020 12:18:58 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deaths</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3/1/2020</td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td>Cyprus</td>\n",
       "      <td>35.126400</td>\n",
       "      <td>33.429900</td>\n",
       "      <td>6/4/2020 12:18:58 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Confirmed</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4/21/2020</td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>17.060800</td>\n",
       "      <td>-61.796400</td>\n",
       "      <td>6/4/2020 12:18:58 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deaths</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>5/11/2020</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>N/A</td>\n",
       "      <td></td>\n",
       "      <td>Thailand</td>\n",
       "      <td>15.870032</td>\n",
       "      <td>100.992541</td>\n",
       "      <td>6/4/2020 12:18:58 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_Type  Cases  Difference       Date       Country_Region  \\\n",
       "0  Confirmed      6           0  5/22/2020       Western Sahara   \n",
       "1  Confirmed      0           0   2/3/2020          Switzerland   \n",
       "2     Deaths      0           0   3/1/2020               Cyprus   \n",
       "3  Confirmed     23           0  4/21/2020  Antigua and Barbuda   \n",
       "4     Deaths     56           0  5/11/2020             Thailand   \n",
       "\n",
       "  Province_State Admin2         Combined_Key        Lat        Long  \\\n",
       "0            N/A              Western Sahara  24.215500  -12.885800   \n",
       "1            N/A                 Switzerland  46.818200    8.227500   \n",
       "2            N/A                      Cyprus  35.126400   33.429900   \n",
       "3            N/A         Antigua and Barbuda  17.060800  -61.796400   \n",
       "4            N/A                    Thailand  15.870032  100.992541   \n",
       "\n",
       "      Prep_Flow_Runtime  FIPS  \n",
       "0  6/4/2020 12:18:58 AM   NaN  \n",
       "1  6/4/2020 12:18:58 AM   NaN  \n",
       "2  6/4/2020 12:18:58 AM   NaN  \n",
       "3  6/4/2020 12:18:58 AM   NaN  \n",
       "4  6/4/2020 12:18:58 AM   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the Json pulled from AWS data lake (this will be used later as well)\n",
    "Json1 = 'part-00000-00f0e506-3bdc-4621-bbc9-e9510a1e84de-c000.json'\n",
    "Json2 = 'part-00001-00f0e506-3bdc-4621-bbc9-e9510a1e84de-c000.json'\n",
    "Json3 = 'part-00002-00f0e506-3bdc-4621-bbc9-e9510a1e84de-c000.json'\n",
    "\n",
    "\n",
    "jsonList = []\n",
    "for line in open(Json1, 'r'):\n",
    "    jsonList.append(json.loads(line))\n",
    "\n",
    "# Converts list to DataFrame for easy exploration\n",
    "df = pd.DataFrame(jsonList)  \n",
    "\n",
    "# Reading top few lines of Tableau's data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows present in Tableau John Hopkins Data: 320281\n",
      "Total number of rows deduplicated: 0\n"
     ]
    }
   ],
   "source": [
    "# First checking row count and then de duplicating to see if this will be a necessary part of clean up later\n",
    "# Keep in mind that this is only one of the Json files as sample data\n",
    "rowNum = len(df.index)\n",
    "print('Number of rows present in Tableau John Hopkins Data:', rowNum)\n",
    "df.drop_duplicates() \n",
    "rowNum2 = len(df.index)\n",
    "print('Total number of rows deduplicated:', rowNum-rowNum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tableau and John Hopkins Data Assessment\n",
    "\n",
    "After careful analysis I also feel confident that most of the Tableau and John Hopkins data doesn't have many data quality issues. Again the primary concern is null values in a few columns. What appears to be missing is when the state or county isn't assigned a situation occurs which subsequently creates null values for county FIPS, lat and long. This isn't terrible since most of the data will be used as aggregations down the line in normalized form. But this will also need to be handled to ensure a good composite key I will be limiting filtering by the United State as part of my scope which will be part the transformation process of the ETL logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Documenting steps necessary to clean the data:\n",
    "\n",
    "Since most of the data seems to be in good condition, here are the few steps I will be taking in the ETL process to clean it up.\n",
    "\n",
    "1. First I will filter all files by the United States, which will reduce my scope, most due to the fact that I reside in the United States.\n",
    "\n",
    "2. I will then remove a hand full (closer to 100) of from the Google Mobility Data that are located in the United State but without state information.\n",
    "\n",
    "3. I will also remove the Combined_Key column from the John Hopkins data, as there will a better normalized composite key to be used later with just state and county combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Mapping out the conceptual data model and explaining why I am choosing that model\n",
    "\n",
    "For ease of use and a little effort needed for transformation I have decided to use a data warehouse on an AWS Redshift cluster to store the data.\n",
    "\n",
    "I am choosing a three table data schema in order to get the data in a factor NF3 (Normalized Form 3) if end users would hypothetically wanted to use this framework to perform analytic queries on the data. The tables end tables will be structured in the following manner:\n",
    "\n",
    "1. One table will be the fact table which will be called demographics. This table will be contain the attributes Country, State, a compositekey of the two just mentioned, County information as well as FIPS and Latitude and Longitude. The County and State information will be a collection from the Json and CSVs in order to ensure all keys will be captured from both tables.\n",
    "2. The second table will be called covidCaseData and will manipulated from the Tableau/John Hopkins collection of data. This table will include the Case_Type, Cases, Dates, and a compositeKey of State and County.\n",
    "3. The last table will be comprised of the Google Mobility data and called googleMobility. The County and State will be combined in order to create a compositeKey. In addition all of the change in baseline columns will be used in order to provide an end user the capability of creating appropriate analytic queries. \n",
    "\n",
    "\n",
    "You can view the architecture on the read me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Listing the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "First I will create a cluster in redshift and fill out the configuration file as appropriate. While using the dl.cfg file to move data from the download to S3, the other configuration is entitled dwh.cfg for the ETL process itself. Generally speaking both have the same information and will need changed to run the code to your specifications. However they were done with two files as I iterated through this process.\n",
    "\n",
    "1. After getting the correct environment and configuration in place, I will create all the necessary tables in Redshift and their corresponding schemas. This includes the three end of scope deliverables mentioned above: demographics; covidCaseData; googleMobility. It also includes two staging tables in Redshift in order to easily perform the transformation. \n",
    "2. From there I will perform all the transforms mentioned previously as well as performing the appropriate insert statements.\n",
    "3. I will then execute data quality checks to ensure that all the data is loaded into the table accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning initial transformation process....\n",
      "Initial tranformation complete!\n",
      "Loading local files into AWS S3 bucket....\n",
      "\n",
      " Moving Global Mobility CSV to S3 from local location....\n",
      "\n",
      " Global Mobility CSV to S3 Complete!\n",
      "\n",
      " Moving John Hopkins 1 to S3 from local location....\n",
      "\n",
      " John Hopkins 1 to S3 Complete!\n",
      "\n",
      " Moving John Hopkins 2 to S3 from local location....\n",
      "\n",
      " John Hopkins 2 to S3 Complete!\n",
      "\n",
      " Moving John Hopkins 3 to S3 from local location....\n",
      "\n",
      " John Hopkins 3 to S3 Complete!\n",
      "Dropping and Creating Tables in Redshift.\n",
      "Created Tables in Redshift without errors!\n",
      "Executing ETL, copying data into staging tables and inserting into final tables.\n",
      "Executing ETL complete without errors!\n"
     ]
    }
   ],
   "source": [
    "# Code can be executed here. I have created some other modules at the bottom in order to keep the code clean. \n",
    "# You can view them in this folder.\n",
    "# You can execute each module to perform a function mentioned above.\n",
    "\n",
    "\n",
    "################################################  Transform  ##############################################################\n",
    "# Because Redshift has a $ MB bite limit for Json files, \n",
    "# I have decided to transpose the Json into CSV in order to make a seamless load process\n",
    "print('Beginning initial transformation process....')\n",
    "\n",
    "# Defines bucket location\n",
    "# NOTE YOU NEED TO CHANGE THIS TO YOUR OWN S3 BUCKET TO GET THE CODE TO WORK APPROPRIATELY\n",
    "S3 = r'capstone-nano-degree-bucket'\n",
    "\n",
    "# ALSO NOTE YOU WILL NEED TO CHANGE THE CONFIGURATION FILES (dl.cfg and dwh.cfg) TO YOUR AWS CREDENTIALS\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id= config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key= config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    ")\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "\n",
    "# Converts list to DataFrame for easy csv transformtion \n",
    "# Peletes combined key (commas are no good in csv)\n",
    "# Places in temp folder\n",
    "# This file in particular needs the columns rearragned to match the other two files\n",
    "jsonList = []\n",
    "for line in open(Json1, 'r'):\n",
    "    jsonList.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(jsonList)  \n",
    "del df['Combined_Key']\n",
    "df = df[['Case_Type','Cases','Difference','Date','Country_Region','Province_State','Admin2','FIPS','Lat','Long','Prep_Flow_Runtime']]\n",
    "df.to_csv(currentDir + r\"/temp/JohnHopkins1.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Converts list to DataFrame for easy csv transformtion \n",
    "# Peletes combined key (commas are no good in csv)\n",
    "# Places in temp folder\n",
    "jsonList2 = []\n",
    "for line in open(Json2, 'r'):\n",
    "    jsonList2.append(json.loads(line))\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(jsonList2)  \n",
    "del df2['Combined_Key']\n",
    "df2.to_csv(currentDir + r\"/temp/JohnHopkins2.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Converts list to DataFrame for easy csv transformtion \n",
    "# Peletes combined key (commas are no good in csv)\n",
    "# Places in temp folder\n",
    "jsonList3 = []\n",
    "for line in open(Json3, 'r'):\n",
    "    jsonList3.append(json.loads(line))\n",
    "\n",
    "df3 = pd.DataFrame(jsonList3) \n",
    "del df3['Combined_Key']\n",
    "df3.to_csv(currentDir + r\"/temp/JohnHopkins3.csv\", index=False)\n",
    "print('Initial tranformation complete!')\n",
    "\n",
    "##############################################Inserts tranform into S3 ###################################################\n",
    "print('Loading local files into AWS S3 bucket....')\n",
    "\n",
    "# Loads Global Mobility CSV\n",
    "print(\"\\n Moving Global Mobility CSV to S3 from local location....\")\n",
    "s3.meta.client.upload_file(Filename = currentDir + r\"\\Global_Mobility_Report.csv\", Bucket = S3 , Key = r'Global_Mobility_Report.csv')\n",
    "print(\"\\n Global Mobility CSV to S3 Complete!\")\n",
    "\n",
    "\n",
    "# Loads John Hopkins 1\n",
    "print(\"\\n Moving John Hopkins 1 to S3 from local location....\")\n",
    "s3.meta.client.upload_file(Filename = currentDir + r\"\\temp\\JohnHopkins1.csv\", Bucket = S3 , Key = r'JohnHopkins1.CSV')\n",
    "print(\"\\n John Hopkins 1 to S3 Complete!\")\n",
    "\n",
    "\n",
    "# Loads John Hopkins 2\n",
    "print(\"\\n Moving John Hopkins 2 to S3 from local location....\")\n",
    "s3.meta.client.upload_file(Filename = currentDir + r\"\\temp/JohnHopkins2.csv\", Bucket = S3 , Key = r'JohnHopkins2.CSV')\n",
    "print(\"\\n John Hopkins 2 to S3 Complete!\")\n",
    "\n",
    "\n",
    "# Loads John Hopkins 3\n",
    "print(\"\\n Moving John Hopkins 3 to S3 from local location....\")\n",
    "s3.meta.client.upload_file(Filename = currentDir + r\"\\temp/JohnHopkins3.csv\", Bucket = S3 , Key = r'JohnHopkins3.CSV')\n",
    "print(\"\\n John Hopkins 3 to S3 Complete!\")\n",
    "\n",
    "\n",
    "# Executes intial drop and create tables on the RedShift cluster\n",
    "print(\"Dropping and Creating Tables in Redshift.\")\n",
    "exec(open(\"create_tables.py\").read())\n",
    "print(\"Created Tables in Redshift without errors!\")\n",
    "\n",
    "\n",
    "# Executes the ETL process\n",
    "print(\"Executing ETL, copying data into staging tables and inserting into final tables.\")\n",
    "exec(open(\"etl.py\").read())\n",
    "print(\"Executing ETL complete without errors!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks I'll perform to ensure the pipeline ran as expected will be a simple counting on the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staging_covidCase\n",
      "\n",
      "-- 943628\n",
      "staging_googleMobility\n",
      "\n",
      "-- 477322\n",
      "demographics\n",
      "\n",
      "-- 3259\n",
      "covidCaseData\n",
      "\n",
      "-- 873412\n",
      "googleMobility\n",
      "\n",
      "-- 282051\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here by creating a unit test count function to ensure that there were the approximate rows accounted for\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "for counting_query in counting_audit_queries:\n",
    "        print(counting_query[26:])\n",
    "        cur.execute(counting_query)\n",
    "        analysis = cur.fetchone()\n",
    "\n",
    "        for row in analysis:\n",
    "            print(\"--\", row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "You can view the dictionary on the read me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Completing Project Write Up\t\n",
    "   \n",
    "   The overall goal of this project was to gather and combine some interesting sub-sets of Covid-19 data in order to get a data warehouse solution for later analysis. As a recap, the technologies that I used for the project were PostgreSQL, AWS Redshift, AWS S3 buckets and Python to create a desirable solution. Because there was less transformation, I decided to use these technologies instead of Spark or Airflow, which support a more robust transformation process.\n",
    "\t\n",
    "   The data itself should probably be updated on a weekly basis, which gives time for the collection to add enough data that would be pertinent additions to the data warehouse store. This is easily accomplished by walking through the Jupyter notebook file provided, with step by step instructions. As requested by Udacity, under the following scenarios I would tackle things differently:\n",
    "    \n",
    "###### The data was increased by 100x\n",
    "If this were the case I would probably include Spark's distributed style framework because even though the transformations seem light, a hundred million rows would pose quite a different scenario for the data that I have collected. \n",
    "\n",
    "\n",
    "###### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "I would probably try to find a source API for the data so that the data could be pulled down easier without having to download a file. In addition, I would use a scheduler like Window Scheduler to execute my converted notebook to python packages. The best choice however would be to use AirFlow in order to schedule the jobs within the DAGs themselves.\n",
    "\n",
    "###### The database needed to be accessed by 100+ people\n",
    "Redshift is pretty expensive if there were going to be several users. I would probably explore AWS Athena which is a cost per query which would generally be the better value. \n",
    "\n",
    "\n",
    "I hope you have enjoyed my project and the write up, I certainly have enjoyed completing my Udacity Data Engineer Nano Degree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}